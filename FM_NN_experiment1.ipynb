{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "90a38db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random as rand\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8630789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Disentangler(nn.Module): \n",
    "    def __init__(self,encoder,decoder, transnet):\n",
    "        super(Disentangler,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.transnet = transnet #estimates trans parameters, contains exponential weights, creates matrices\n",
    "        \n",
    "    def forward(self,x, x0=None):\n",
    "        if x0 == None:\n",
    "            y = self.encoder(x)\n",
    "            s = torch.zeros(x.size(0), self.encoder.latent_dim)\n",
    "        else:\n",
    "            y, s = self.transnet(x,x0)\n",
    "        z = self.decoder(y)\n",
    "        return z,y,s\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, og_dim, latent_dim): #if images are nXn, og_dim = n^2.\n",
    "        assert latent_dim <= og_dim, 'latent space must have lower dimension'\n",
    "        super(Encoder,self).__init__()\n",
    "        self.og_dim = og_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fc1 = nn.Linear(og_dim, max(latent_dim, og_dim//16))\n",
    "        self.fc2 = nn.Linear(max(latent_dim, og_dim//16), latent_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):  \n",
    "    def __init__(self, og_dim, latent_dim):\n",
    "        assert latent_dim <= og_dim, 'latent space must have lower dimension'\n",
    "        super(Decoder,self).__init__()\n",
    "        self.og_dim = og_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fc1 = nn.Linear(latent_dim, max(latent_dim, og_dim//16))\n",
    "        self.fc2 = nn.Linear(max(latent_dim, og_dim//16), og_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "class Transnet(nn.Module):\n",
    "    def __init__(self, og_dim, latent_dim, trans_dim, k_sparse):\n",
    "        super(Transnet,self).__init__()\n",
    "        assert latent_dim <= og_dim, 'latent space must have lower dimension'\n",
    "        assert trans_dim <= latent_dim, 'translation dimension must be subspace'\n",
    "        self.og_dim = og_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.trans_dim = trans_dim\n",
    "        ttl_dim = og_dim + latent_dim\n",
    "        self.ttl_dim = ttl_dim\n",
    "        self.k_sparse = k_sparse\n",
    "        self.fc1 = nn.Linear(ttl_dim, max(latent_dim, ttl_dim//16))\n",
    "        self.fc2 = nn.Linear(max(latent_dim, ttl_dim//16), max(latent_dim, ttl_dim//32))\n",
    "        self.fc3 = nn.Linear(max(latent_dim, ttl_dim//32), trans_dim)\n",
    "        \n",
    "    def forward(self,x,x0):\n",
    "        x1 = torch.cat((x,x0),dim = 1) #create (B, N+M) tensor\n",
    "        x1 = self.fc1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.fc2(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.fc3(x1)\n",
    "        x0[:,:self.trans_dim] += x1\n",
    "        return x0, x1\n",
    "        \n",
    "class Autoencoder(nn.Module): \n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self,x, x0=None):    \n",
    "        x=self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8aa69a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(og_dim, latent_dim, trans_dim, k_sparse=1):\n",
    "    enc = Encoder(og_dim, latent_dim)\n",
    "    dec = Decoder(og_dim, latent_dim)\n",
    "    trans = Transnet(og_dim, latent_dim, trans_dim, k_sparse)\n",
    "    model = Disentangler(enc,dec,trans)\n",
    "    return model\n",
    "\n",
    "def make_autoenc(og_dim,latent_dim):\n",
    "    enc = Encoder(og_dim, latent_dim)\n",
    "    dec = Decoder(og_dim, latent_dim)\n",
    "    model = Autoencoder(enc,dec)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "93631fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f9ab97fb370>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            torch.autograd.set_detect_anomaly(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "44fec6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(print_interval, model, device, train_loader, optimizer, epoch, movie_len, transform_set, beta = .7): #transforms is \n",
    "    model.train()\n",
    "    for epoch in range(epoch):\n",
    "        train_encoder = True\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#             if batch_idx % 4 == 0:\n",
    "#                 if train_encoder == True:\n",
    "#                     for param in model.parameters():\n",
    "#                         param.requires_grad = True\n",
    "#                     for param in model.encoder.parameters():\n",
    "#                         param.requires_grad = False\n",
    "#                     train_encoder == False\n",
    "#                 else:\n",
    "#                     for param in model.parameters():\n",
    "#                         param.requires_grad = False\n",
    "#                     for param in model.encoder.parameters():\n",
    "#                         param.requires_grad = True\n",
    "#                     train_encoder == True\n",
    "            loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            for i in range(movie_len):\n",
    "                if i == 0:\n",
    "                    transform = rand.choice(transform_set)\n",
    "                    prev_frame = curr_frame = data\n",
    "                    curr_frame = curr_frame.flatten(1).to(device)\n",
    "                    output, latent_rep, trans_par = model(curr_frame)\n",
    "                    latent_rep = latent_rep.detach().clone()\n",
    "                    latent_rep = latent_rep.to(device)\n",
    "                else:\n",
    "                    curr_frame = transform(prev_frame)\n",
    "                    prev_frame = curr_frame\n",
    "                    curr_frame = curr_frame.flatten(1).to(device)\n",
    "                    output, out_rep, trans_par = model(curr_frame, latent_rep)\n",
    "                loss += (beta**i)*(F.mse_loss(output, curr_frame) + 5e-2*(1/50)*torch.norm(trans_par,1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % print_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3a46e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enc(print_interval, model, device, train_loader, optimizer, epoch, movie_len, transform_set): #transforms is \n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        for i in range(movie_len):\n",
    "            optimizer.zero_grad()\n",
    "            if i == 0:\n",
    "                prev_frame =  curr_frame = data\n",
    "                transform = rand.choice(transform_set)\n",
    "                curr_frame = curr_frame.flatten(1).to(device)\n",
    "            else:\n",
    "                curr_frame = transform(prev_frame)\n",
    "                prev_frame = curr_frame\n",
    "                curr_frame = curr_frame.flatten(1).to(device)\n",
    "            output = model(curr_frame)\n",
    "            loss = F.mse_loss(output, curr_frame)# + torch.norm(trans_par,1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if batch_idx % print_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "305250de",
   "metadata": {},
   "outputs": [],
   "source": [
    "hor_trans = transforms.Compose(\n",
    "    [transforms.RandomAffine(0, translate = (.1,0)),\n",
    "     transforms.Normalize(.3,.3)])\n",
    "\n",
    "ver_trans = transforms.Compose(\n",
    "    [transforms.RandomAffine(0,translate = (0,.1)),\n",
    "     transforms.Normalize(.3,.3)])\n",
    "\n",
    "\n",
    "transform_set = [hor_trans,ver_trans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0a655a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(.3,.3)]\n",
    ")\n",
    "batch_size = 50\n",
    "#(Down)Load MNIST\n",
    "data_set = datasets.MNIST(root='./data', train=True, download=False, transform=loader_transform)\n",
    "\n",
    "#Create data loader\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size = batch_size, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8264d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dis = make_model(28**2, latent_dim= 16, trans_dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "484a61b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dis = make_autoenc(28**2, latent_dim= 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1e18457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(1)\n",
    "torch.cuda.set_device(1)\n",
    "model_dis = model_dis.to(device)\n",
    "optimizer = torch.optim.Adam(model_dis.parameters(), lr=0.001) #e-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "232869d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [50, 16]], which is output 0 of torch::autograd::CopySlices, is at version 4; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-0536ba99d4a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-111-2a052ee43344>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(print_interval, model, device, train_loader, optimizer, epoch, movie_len, transform_set, beta)\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_par\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_frame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5e-2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_par\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [50, 16]], which is output 0 of torch::autograd::CopySlices, is at version 4; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "train(200, model_dis, device, data_loader, optimizer, epoch = 10, movie_len = 3, transform_set = transform_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "4ea9c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testf(x,y,z):\n",
    "    return x,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "c1988982",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, _ , _ = testf(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f30c596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = B = torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7839a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = B.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "ce9e8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "d4c2a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss += torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6a11478c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7183, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 2.7183, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 2.7183, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 2.7183]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand.choice(list_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3f3bcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ebaa34b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "86d1ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "B= torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7f2261b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3960, 0.7971],\n",
       "        [0.6771, 0.2819],\n",
       "        [0.4241, 0.7296]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "854a8486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3960, 0.7971, 0.3008, 0.6639],\n",
       "        [0.6771, 0.2819, 0.3756, 0.3071],\n",
       "        [0.4241, 0.7296, 0.6412, 0.2367]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "41981530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3960, 0.7971, 0.3008, 0.6639],\n",
       "        [0.6771, 0.2819, 0.3756, 0.3071],\n",
       "        [0.4241, 0.7296, 0.6412, 0.2367]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f06c5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.zeros(3,2)\n",
    "B = torch.zeros(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "07696894",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.cat((A,B),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0ba45966",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fef5988e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a5313ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3b6a435e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.flatten(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7744cf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bda8c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "27f8729b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a836ed8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "78c215bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a8986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
